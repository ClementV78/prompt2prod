#!/bin/bash

# Script de test complet pour Prompt2Prod
# ExÃ©cute tous les types de tests: unitaires, sÃ©curitÃ©, et intÃ©gration

set -e

echo "ğŸ§ª Starting complete test suite for Prompt2Prod"
echo "================================================"

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_DIR="$(dirname "$SCRIPT_DIR")"

cd "$PROJECT_DIR"

# VÃ©rifier la structure Python
if [ ! -f "requirements.txt" ] || [ ! -f "requirements-test.txt" ]; then
    echo "âŒ Missing requirements files"
    exit 1
fi

# Installer les dÃ©pendances si nÃ©cessaire
echo "ğŸ“¦ Installing dependencies..."
if [ ! -d "venv" ]; then
    echo "ğŸ”§ Creating virtual environment..."
    python -m venv venv
fi

source venv/bin/activate
python -m pip install --upgrade pip
pip install -r requirements.txt
pip install -r requirements-test.txt
pip install pbr  # For bandit

# 1. Tests unitaires
echo ""
echo "ğŸ”¬ Running unit tests..."
PYTHONPATH=$PROJECT_DIR:$PYTHONPATH pytest tests/unit/ -v \
    --cov=src \
    --cov-report=term-missing \
    --cov-report=html:htmlcov \
    --cov-report=xml:coverage.xml \
    --junitxml=test-results.xml

# 2. Tests de sÃ©curitÃ©
echo ""
echo "ğŸ”’ Running security tests..."
PYTHONPATH=$PROJECT_DIR:$PYTHONPATH pytest tests/security/ -v -m security --tb=short

# 3. Scan de sÃ©curitÃ© avec Bandit
echo ""
echo "ğŸ Running Bandit security scan..."
bandit -r src/ -ll --skip B101 || {
    echo "âš ï¸  Security issues found, but continuing..."
}

# 4. Scan des vulnÃ©rabilitÃ©s des dÃ©pendances
echo ""
echo "ğŸ” Checking dependencies for vulnerabilities..."
safety check || {
    echo "âš ï¸  Vulnerability warnings found, review them carefully"
}

# 5. Configuration pour tests d'intÃ©gration LLM
echo ""
echo "âš™ï¸ Configuring LLM backends for integration tests..."

# VÃ©rifier si OpenAI est disponible
if [ -n "$OPENAI_API_KEY" ]; then
    echo "âœ… OpenAI API key found - cloud tests will run"
    OPENAI_AVAILABLE=true
else
    echo "âš ï¸  OpenAI API key not found - cloud tests will be skipped"
    OPENAI_AVAILABLE=false
fi

# Configurer Ollama local si disponible
if command -v ollama &> /dev/null; then
    echo "ğŸ¤– Ollama found - setting up local LLM backend..."
    
    # DÃ©marrer Ollama si pas dÃ©jÃ  lancÃ©
    if ! curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
        echo "ğŸš€ Starting Ollama service..."
        ollama serve >/dev/null 2>&1 &
        OLLAMA_PID=$!
        
        # Attendre que Ollama soit prÃªt
        for i in {1..30}; do
            if curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
                echo "âœ… Ollama service ready"
                break
            fi
            if [ $i -eq 30 ]; then
                echo "âŒ Ollama failed to start"
                kill $OLLAMA_PID 2>/dev/null || true
                OLLAMA_PID=""
                break
            fi
            sleep 1
        done
    fi
    
    # VÃ©rifier/installer un modÃ¨le lÃ©ger pour tests
    if [ -n "$OLLAMA_PID" ] || curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
        if ! curl -s http://localhost:11434/api/tags | grep -q "phi3"; then
            echo "ğŸ“¥ Installing lightweight model for tests (phi3:mini)..."
            ollama pull phi3:mini >/dev/null 2>&1 &
            PULL_PID=$!
            echo "â³ Model download in background..."
        else
            echo "âœ… Test model (phi3:mini) already available"
        fi
        OLLAMA_AVAILABLE=true
    else
        OLLAMA_AVAILABLE=false
    fi
else
    echo "âš ï¸  Ollama not found - local LLM tests will be skipped"
    OLLAMA_AVAILABLE=false
fi

# 6. Tests d'intÃ©gration (si API_URL est dÃ©finie)
if [ -n "$API_URL" ]; then
    echo ""
    echo "ğŸŒ Running integration tests against $API_URL..."
    PYTHONPATH=$PROJECT_DIR:$PYTHONPATH pytest tests/integration/ -v -m integration --tb=short
elif command -v docker &> /dev/null; then
    # Builder l'image si elle n'existe pas
    if ! docker images | grep -q "prompt2prod.*latest"; then
        echo "ğŸ”§ Building Docker image for integration tests..."
        docker build -f docker/Dockerfile -t prompt2prod:latest .
    fi
    echo ""
    echo "ğŸ³ Starting local Docker container for integration tests..."
    
    # ArrÃªter le container s'il existe dÃ©jÃ 
    docker stop prompt2prod-test 2>/dev/null || true
    docker rm prompt2prod-test 2>/dev/null || true
    
    # DÃ©ployer la stack complÃ¨te pour tests d'intÃ©gration
    echo "ğŸš€ Deploying full integration test stack..."
    
    # 1. DÃ©marrer Ollama avec modÃ¨le prÃ©-installÃ©
    echo "ğŸ¤– Setting up Ollama with cached models..."
    if ! pgrep -f "ollama serve" > /dev/null; then
        ollama serve >/dev/null 2>&1 &
        OLLAMA_PID=$!
        sleep 5
    fi
    
    # Installer un modÃ¨le lÃ©ger s'il n'existe pas
    if ! ollama list | grep -q "phi3:mini"; then
        echo "ğŸ“¦ Installing phi3:mini model (lightweight for tests)..."
        ollama pull phi3:mini >/dev/null 2>&1 &
        PULL_PID=$!
        # Ne pas attendre la fin, utiliser le modÃ¨le par dÃ©faut en attendant
    fi
    
    # 2. CrÃ©er un rÃ©seau Docker pour les tests
    docker network create prompt2prod-test 2>/dev/null || true
    
    # 3. DÃ©marrer Mock KGateway pour tests d'intÃ©gration
    echo "ğŸ­ Starting Mock KGateway (simulates real KGateway behavior)..."
    source venv/bin/activate
    python tests/mock_kgateway.py 8080 &
    MOCK_KGATEWAY_PID=$!
    
    # Attendre que le mock soit prÃªt
    echo "â³ Waiting for Mock KGateway to be ready..."
    for i in {1..15}; do
        if curl -f http://localhost:8080/health >/dev/null 2>&1; then
            echo "âœ… Mock KGateway ready"
            break
        fi
        if [ $i -eq 15 ]; then
            echo "âŒ Mock KGateway failed to start"
            kill $MOCK_KGATEWAY_PID 2>/dev/null || true
            exit 1
        fi
        sleep 1
    done
    
    # 4. DÃ©marrer l'application connectÃ©e au Mock KGateway
    echo "ğŸš€ Starting application with Mock KGateway backend..."
    docker run -d --name prompt2prod-test \
        --network prompt2prod-test \
        -p 8888:8000 \
        -e KGATEWAY_ENDPOINT="http://host.docker.internal:8080" \
        -e OLLAMA_HOST="http://host.docker.internal:11434" \
        ${OPENAI_API_KEY:+-e OPENAI_API_KEY="$OPENAI_API_KEY"} \
        --add-host=host.docker.internal:host-gateway \
        prompt2prod:latest
    
    # Attendre que l'API soit prÃªte
    echo "â³ Waiting for API to be ready..."
    for i in {1..30}; do
        if curl -f http://localhost:8888/health >/dev/null 2>&1; then
            echo "âœ… API is ready"
            break
        fi
        if [ $i -eq 30 ]; then
            echo "âŒ API not ready after 30 attempts"
            docker logs prompt2prod-test
            docker stop prompt2prod-test
            docker rm prompt2prod-test
            exit 1
        fi
        sleep 2
    done
    
    # ExÃ©cuter les tests d'intÃ©gration avec variables d'env
    export API_URL="http://localhost:8888"
    ${OPENAI_API_KEY:+export OPENAI_API_KEY="$OPENAI_API_KEY"}
    PYTHONPATH=$PROJECT_DIR:$PYTHONPATH pytest tests/integration/ -v -m integration --tb=short
    
    # Nettoyer
    docker stop prompt2prod-test
    docker rm prompt2prod-test
    
    echo "âœ… Integration tests completed"
    
    # Nettoyer les processus et containers
    if [ -n "$MOCK_KGATEWAY_PID" ]; then
        echo "ğŸ§¹ Stopping Mock KGateway..."
        kill $MOCK_KGATEWAY_PID 2>/dev/null || true
    fi
    if [ -n "$OLLAMA_PID" ]; then
        echo "ğŸ§¹ Stopping Ollama service..."
        kill $OLLAMA_PID 2>/dev/null || true
    fi
    if [ -n "$PULL_PID" ]; then
        kill $PULL_PID 2>/dev/null || true
    fi
    
    # Nettoyer le rÃ©seau Docker
    docker network rm prompt2prod-test 2>/dev/null || true
else
    echo "âš ï¸  Skipping integration tests (no API_URL or Docker image)"
fi

# 6. RÃ©sumÃ©
echo ""
echo "ğŸ“Š Test Summary"
echo "==============="

if [ -f "coverage.xml" ]; then
    # Extraire le pourcentage de couverture
    COVERAGE=$(grep -o 'line-rate="[^"]*"' coverage.xml | head -1 | sed 's/line-rate="//;s/"//' | awk '{printf "%.1f", $1*100}')
    echo "ğŸ“ˆ Code coverage: ${COVERAGE}%"
fi

if [ -f "test-results.xml" ]; then
    TESTS_COUNT=$(grep -o 'tests="[^"]*"' test-results.xml | sed 's/tests="//;s/"//')
    FAILURES=$(grep -o 'failures="[^"]*"' test-results.xml | sed 's/failures="//;s/"//')
    ERRORS=$(grep -o 'errors="[^"]*"' test-results.xml | sed 's/errors="//;s/"//')
    
    echo "ğŸ§ª Tests executed: $TESTS_COUNT"
    echo "âŒ Failures: $FAILURES"
    echo "ğŸ’¥ Errors: $ERRORS"
    
    if [ "$FAILURES" -gt 0 ] || [ "$ERRORS" -gt 0 ]; then
        echo "âŒ Some tests failed!"
        exit 1
    fi
fi

echo ""
echo "ğŸ‰ All tests passed successfully!"
echo ""
echo "ğŸ“ Generated reports:"
echo "  â€¢ HTML coverage: htmlcov/index.html"
echo "  â€¢ XML coverage: coverage.xml"
echo "  â€¢ Test results: test-results.xml"